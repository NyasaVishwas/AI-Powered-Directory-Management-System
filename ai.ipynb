{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1018)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIModule:\n",
    "    def __init__(self):\n",
    "        self.model_path = \"file_classifier.pkl\"\n",
    "        self.vectorizer_path = \"vectorizer.pkl\"\n",
    "\n",
    "        # Load existing model or create a new one\n",
    "        if os.path.exists(self.model_path) and os.path.exists(self.vectorizer_path):\n",
    "            self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "            self.model = joblib.load(self.model_path)\n",
    "        else:\n",
    "            self.vectorizer = None\n",
    "            self.model = None\n",
    "\n",
    "    def extract_features(self, filenames):\n",
    "        \"\"\"Extracts text features from file names.\"\"\"\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        cleaned_filenames = [\" \".join(re.sub(r\"[^a-zA-Z0-9]\", \" \", file).split()) for file in filenames]\n",
    "        filtered_filenames = [\" \".join([word for word in file.split() if word.lower() not in stop_words]) for file in cleaned_filenames]\n",
    "\n",
    "        # Vectorize filenames using TF-IDF\n",
    "        self.vectorizer = TfidfVectorizer(max_features=100)\n",
    "        features = self.vectorizer.fit_transform(filtered_filenames)\n",
    "        return features\n",
    "\n",
    "    def train_model(self, file_list):\n",
    "        \"\"\"Trains a K-Means model to categorize files.\"\"\"\n",
    "        if not file_list:\n",
    "            return \"No files to categorize!\"\n",
    "\n",
    "        features = self.extract_features(file_list)\n",
    "        self.model = KMeans(n_clusters=5, random_state=42)\n",
    "        self.model.fit(features)\n",
    "\n",
    "        # Save the trained model\n",
    "        joblib.dump(self.model, self.model_path)\n",
    "        joblib.dump(self.vectorizer, self.vectorizer_path)\n",
    "\n",
    "        return \"AI Model trained successfully!\"\n",
    "\n",
    "    def categorize_files(self, file_list):\n",
    "        \"\"\"Categorizes files using the trained model.\"\"\"\n",
    "        if not self.model or not self.vectorizer:\n",
    "            return \"Train the model first!\"\n",
    "\n",
    "        features = self.vectorizer.transform(file_list)\n",
    "        predictions = self.model.predict(features)\n",
    "\n",
    "        categorized_files = defaultdict(list)\n",
    "        for i, category in enumerate(predictions):\n",
    "            categorized_files[f\"Category {category + 1}\"].append(file_list[i])\n",
    "\n",
    "        return categorized_files\n",
    "\n",
    "    def predict_file_usage(self, directory):\n",
    "        \"\"\"Predicts file usage patterns based on modification dates.\"\"\"\n",
    "        usage_patterns = defaultdict(int)\n",
    "        current_time = datetime.datetime.now()\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                days_old = (current_time - mod_time).days\n",
    "                if days_old < 30:\n",
    "                    usage_patterns[\"Recent\"] += 1\n",
    "                elif days_old < 90:\n",
    "                    usage_patterns[\"Last 3 Months\"] += 1\n",
    "                else:\n",
    "                    usage_patterns[\"Older\"] += 1\n",
    "\n",
    "        return dict(usage_patterns)\n",
    "\n",
    "    def smart_search(self, directory, query):\n",
    "        \"\"\"Performs smart search using NLP techniques.\"\"\"\n",
    "        results = []\n",
    "        query = query.lower()\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            if query in file.lower():\n",
    "                results.append(file)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ai.ipynb to script\n",
      "[NbConvertApp] Writing 3504 bytes to ai.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script ai.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
